{
    "version": "https://jsonfeed.org/version/1",
    "title": "Or0kit • All posts by \"scrapy\" tag",
    "description": "临渊羡鱼，不如退而结网",
    "home_page_url": "https://Or0kit.github.io",
    "items": [
        {
            "id": "https://or0kit.github.io/Programming/Python/scrapy%E6%A1%86%E6%9E%B6/",
            "url": "https://or0kit.github.io/Programming/Python/scrapy%E6%A1%86%E6%9E%B6/",
            "title": "scrapy框架",
            "date_published": "2020-12-28T09:47:19.000Z",
            "content_html": "<h1 id=\"什么是scrapy\"><a class=\"anchor\" href=\"#什么是scrapy\">#</a> 什么是 scrapy</h1>\n<font color=\"#00CED1\">\n<p>Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说，网络抓取) 所设计的，也可以应用在获取 API 所返回的数据 (例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy 其实是 Search+Python。Scrapy 使用 Twisted 这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。</p>\n</font>\n<h1 id=\"scrapy架构\"><a class=\"anchor\" href=\"#scrapy架构\">#</a> scrapy 架构</h1>\n<h2 id=\"scrapy整体结构\"><a class=\"anchor\" href=\"#scrapy整体结构\">#</a> Scrapy 整体结构</h2>\n<font color=\"#00CED1\">\n<p><strong>1、引擎 (Scrapy Engine)</strong><br />\n 用来处理整个系统的数据流处理，触发事务。</p>\n<p><strong>2、调度器 (Scheduler)</strong><br />\n 用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。</p>\n<p><strong>3、下载器 (Downloader)</strong><br />\n 用于下载网页内容，并将网页内容返回给蜘蛛。</p>\n<p><strong>4、蜘蛛 (Spiders)</strong><br />\n 蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析 response 并提取 item (即获取到的 item) 或额外跟进的 URL 的类。<br />\n每个 spider 负责处理一个特定 (或一些) 网站。蜘蛛的整个抓取流程（周期）是这样的：</p>\n<ol>\n<li>首先获取第一个 URL 的初始请求，当请求返回后调取一个回调函数。第一个请求是通过调用 start_requests () 方法。该方法默认从 start_urls 中的 Url 中生成请求，并执行解析来调用回调函数。</li>\n<li>在回调函数中，你可以解析网页响应并返回项目对象和请求对象或两者的迭代。这些请求也将包含一个回调，然后被 Scrapy 下载，然后有指定的回调处理。</li>\n<li>在回调函数中，你解析网站的内容，同程使用的是 Xpath 选择器（但是你也可以使用 BeautifuSoup, lxml 或其他任何你喜欢的程序），并生成解析的数据项。</li>\n<li>最后，从蜘蛛返回的项目通常会进驻到项目管道。</li>\n</ol>\n<p><strong>5、项目管道 (Item Pipeline)</strong><br />\n 主要责任是负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。每个项目管道的组件都是有一个简单的方法组成的 Python 类。他们获取了项目并执行他们的方法，同时他们还需要确定的是是否需要在项目管道中继续执行下一步或是直接丢弃掉不处理。<br />\n项目管道通常执行的过程有：</p>\n<ol>\n<li>清洗 HTML 数据</li>\n<li>验证解析到的数据（检查项目是否包含必要的字段）</li>\n<li>检查是否是重复数据（如果重复就删除）</li>\n<li>将解析到的数据存储到数据库中</li>\n</ol>\n<p><strong>6、下载器中间件 (Downloader Middlewares)</strong><br />\n 位于 Scrapy 引擎和下载器之间的钩子框架，主要是处理 Scrapy 引擎与下载器之间的请求及响应。它提供了一个自定义的代码的方式来拓展 Scrapy 的功能。下载中间器是一个处理请求和响应的钩子框架。他是轻量级的，对 Scrapy 尽享全局控制的底层的系统。</p>\n<p><strong>7、蜘蛛中间件 (Spider Middlewares)</strong><br />\n 介于 Scrapy 引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。它提供一个自定义代码的方式来拓展 Scrapy 的功能。蛛中间件是一个挂接到 Scrapy 的蜘蛛处理机制的框架，你可以插入自定义的代码来处理发送给蜘蛛的请求和返回蜘蛛获取的响应内容和项目。</p>\n<p><strong>8、调度中间件 (Scheduler Middlewares)</strong><br />\n 介于 Scrapy 引擎和调度之间的中间件，从 Scrapy 引擎发送到调度的请求和响应。他提供了一个自定义的代码来拓展 Scrapy 的功能。</p>\n<p><strong>数据处理流程</strong><br />\n<img data-src=\"image001.jpg\" alt=\"\" /></p>\n<p>如图所示，显示的是 Scrapy 爬虫执行流程，绿线是数据流向，首先从初始 URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，Spider 分析出来的结果有两种：一种是需要进一步抓取的链接，例如之前分析的 “下一页” 的链接，这些东西会被传回 Scheduler；另一种是需要保存的数据，它们则被送到 Item Pipeline 那里，那是对数据进行后期处理（详细分析、过滤、存储等）的地方。另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。</p>\n<p>Scrapy 中的数据流由执行引擎控制，其过程如下:</p>\n<ol>\n<li>引擎打开一个网站 (open a domain)，找到处理该网站的 Spider 并向该 spider 请求第一个要爬取的 URL (s)。</li>\n<li>引擎从 Spider 中获取到第一个要爬取的 URL 并在调度器 (Scheduler) 以 Request 调度。</li>\n<li>引擎向调度器请求下一个要爬取的 URL。</li>\n<li>调度器返回下一个要爬取的 URL 给引擎，引擎将 URL 通过下载中间件 (请求 (request) 方向) 转发给下载器 (Downloader)。</li>\n<li>一旦页面下载完毕，下载器生成一个该页面的 Response, 并将其通过下载中间件 (返回 (response) 方向) 发送给引擎。</li>\n<li>引擎从下载器中接收到 Response 并通过 Spider 中间件 (输入方向) 发送给 Spider 处理。</li>\n<li>Spider 处理 Response 并返回爬取到的 Item 及 (跟进的) 新的 Request 给引擎。</li>\n<li>引擎将 (Spider 返回的) 爬取到的 Item 给 Item Pipeline，将 (Spider 返回的) Request 给调度器。</li>\n<li>(从第二步) 重复直到调度器中没有更多地 request，引擎关闭该网站</li>\n</ol>\n</font>\n<h2 id=\"scrapy命令行工具\"><a class=\"anchor\" href=\"#scrapy命令行工具\">#</a> Scrapy 命令行工具</h2>\n<font color=\"green\">\n<p>Scrapy 是通过 scrapy 命令行工具进行控制的。这里我们称之为 “Scrapy tool” 以用来和子命令进行区分。对于子命令，我们称为 “command” 或者 “Scrapy commands”。Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项</p>\n</font>\n<h2 id=\"默认的scrapy-项目结构\"><a class=\"anchor\" href=\"#默认的scrapy-项目结构\">#</a> 默认的 Scrapy 项目结构</h2>\n<font color=\"DeepSkyBlue\">\n<p>在开始对命令行工具以及子命令的探索前，让我们首先了解一下 Scrapy 的项目的目录结构。虽然可以被修改，但所有的 Scrapy 项目默认有类似于下边的文件结构:</p>\n<pre><code>scrapy.cfg\n    myproject/\n        __init__.py\n        items.py\n        middlewares.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            spider1.py\n            spider2.py\n            ...\n</code></pre>\n<ol>\n<li>scrapy.cfg<br />\n 存放的目录被认为是项目的根目录。该文件中包含 python 模块名的字段定义了项目的设置。</li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL2l0ZW1zLnB5\">items.py</span><br />\n 该文件中包含了 scrapy 数据容器模型代码。<br />\nItem 对象是种简单的容器，保存了爬取到得数据。其提供了类似于词典 (dictionary-like) 的 API 以及用于声明可用字段的简单语法。</li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL21pZGRsZXdhcmVzLnB5\">middlewares.py</span><br />\n 该文件中包含下载器中间件和爬虫中间件模型代码。<br />\n下载器中间件是介于 Scrapy 的 request/response 处理的钩子框架。是用于全局修改 Scrapy request 和 response 的一个轻量、底层的系统。<br />\n爬虫中间件是介入到 Scrapy 的 spider 处理机制的钩子框架，您可以添加代码来处理发送给 Spiders 的 response 及 spider 产生的 item 和 request。。</li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3BpcGVsaW5lcy5weQ==\">pipelines.py</span><br />\n 每个管道组件是实现了简单方法的 Python 类。<br />\n他们接收到 Item 并通过它执行一些行为，同时也决定此 Item 是否继续通过后续的管道组件，或是被丢弃而不再进行处理。</li>\n<li><span class=\"exturl\" data-url=\"aHR0cDovL3NldHRpbmdzLnB5\">settings.py</span><br />\nScrapy 设定 (settings) 提供了定制 Scrapy 组件的方法。<br />\n您可以控制包括核心 (core)，插件 (extension)，pipeline 及 spider 组件。</li>\n</ol>\n</font>\n<h1 id=\"编写第一个scrapy爬虫\"><a class=\"anchor\" href=\"#编写第一个scrapy爬虫\">#</a> 编写第一个 Scrapy 爬虫</h1>\n<font color=\"BlueViolet\">\n<p>任务:</p>\n<ol>\n<li>创建一个 Scrapy 项目</li>\n<li>定义提取的 Item</li>\n<li>编写爬取网站的 spider 并提取 Item</li>\n<li>编写 Item Pipeline 来存储提取到的 Item (即数据)</li>\n</ol>\n</font>\n<h2 id=\"创建项目\"><a class=\"anchor\" href=\"#创建项目\">#</a> 创建项目</h2>\n<font color=\"BlueViolet\">\n<p>在开始爬取之前，您必须创建一个新的 Scrapy 项目。<br />\n进入您打算存储代码的目录中，运行下列命令: <code>scrapy startproject tutorial</code> <br />\n 该命令将会创建包含下列内容的 tutorial 目录:</p>\n<pre><code>tutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n</code></pre>\n<p>这些文件分别是:</p>\n<ul>\n<li>scrapy.cfg: 项目的配置文件</li>\n<li>tutorial/: 该项目的 python 模块。之后您将在此加入代码。</li>\n<li>tutorial/items.py: 项目中的 item 文件。</li>\n<li>tutorial/pipelines.py: 项目中的 pipelines 文件。</li>\n<li>tutorial/settings.py: 项目的设置文件。</li>\n<li>tutorial/spiders/: 放置 spider 代码的目录</li>\n</ul>\n</font>\n<h2 id=\"定义item\"><a class=\"anchor\" href=\"#定义item\">#</a> 定义 Item</h2>\n<font color=\"BlueViolet\">\n<p>Item 是保存爬取到的数据的容器；其使用方法和 python 字典类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。<br />\n类似在 ORM 中做的一样，您可以通过创建一个 scrapy.Item 类，并且定义类型为 scrapy.Field 的类属性来定义一个 Item。(如果不了解 ORM, 不用担心，您会发现这个步骤非常简单)<br />\n 首先根据需要从 dmoz.org 获取到的数据对 item 进行建模。我们需要从 dmoz 中获取名字，url，以及网站的描述。对此，在 item 中定义相应的字段。编辑 tutorial 目录中的 items.py 文件。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>items<span class=\"token punctuation\">.</span>py</pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">import</span> scrapyclass </pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>DmozItem<span class=\"token punctuation\">(</span>scrapy<span class=\"token punctuation\">.</span>Item<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    title<span class=\"token operator\">=</span>scrapy<span class=\"token punctuation\">.</span>Field<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    link<span class=\"token operator\">=</span>scrapy<span class=\"token punctuation\">.</span>Field<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    desc<span class=\"token operator\">=</span>scrapy<span class=\"token punctuation\">.</span>Field<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>代码一开始这看起来可能有点复杂，但是通过定义 item，您可以很方便的使用 Scrapy 的其他方法。而这些方法需要知道您的 item 的定义。</p>\n</font>\n<h2 id=\"spider爬虫\"><a class=\"anchor\" href=\"#spider爬虫\">#</a> Spider 爬虫</h2>\n<font color=\"BlueViolet\">\n<p>Spider 是用户编写用于从单个网站 (或者一些网站) 爬取数据的类。<br />\n其包含了一个用于下载的初始 URL，如何跟进网页中的链接以及如何分析页面中的内容，提取生成 item 的方法。</p>\n<p>为了创建一个 Spider，您必须继承 scrapy.Spider 类，且定义以下三个属性:</p>\n<ul>\n<li>name: 用于区别 Spider。该名字必须是唯一的，您不可以为不同的 Spider 设定相同的名字。</li>\n<li>start_urls: 包含了 Spider 在启动时进行爬取的 url 列表。因此，第一个被获取到的页面将是其中之一。后续的 URL 则从初始的 URL 获取到的数据中提取。</li>\n<li>parse () 是 spider 的一个方法。被调用时，每个初始 URL 完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。该方法负责解析返回的数据 (response data)，提取数据 (生成 item) 以及生成需要进一步处理的 URL 的 Request 对象。</li>\n</ul>\n<p>也可以使用命令行创建一个 Spider。比如要生成 Quotes 这个 Spider，可以执行如下命令：<br />\n <code>cd 项目目录</code> <br />\n <code>scrapy genspider quotes quotes.toscrape.com</code> <br />\n 语法格式<br />\n <code>scrapy genspider [options] &lt;name&gt; &lt;domain&gt;</code></p>\n<p>以下为我们的第一个 Spider 代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中，如 Code 3-21 所示:</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># -*-coding: utf-8 -*-</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">import</span> scrapyclass </pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>DmozSpider<span class=\"token punctuation\">(</span>scrapy<span class=\"token punctuation\">.</span>Spider<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    name <span class=\"token operator\">=</span> <span class=\"token string\">'dmoz'</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    allowed_domains <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'dmoz.org'</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    start_urls <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'http://www.dmoz.org/Computers/Programming/Langurages/Python/Books'</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token string\">'http://www.dmoz.org/Computers/Programming/Langurages/Python/Resources'</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre></pre></td></tr><tr><td data-num=\"12\"></td><td><pre></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">parse</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> response<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>    filename<span class=\"token operator\">=</span>response<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\"/\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>filename<span class=\"token punctuation\">,</span><span class=\"token string\">'wb'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>        f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span>response<span class=\"token punctuation\">.</span>body<span class=\"token punctuation\">)</span></pre></td></tr></table></figure></font>\n<h2 id=\"爬虫爬取\"><a class=\"anchor\" href=\"#爬虫爬取\">#</a> 爬虫爬取</h2>\n<font color=\"BlueViolet\">\n<p>进入项目的根目录，执行下列命令启动 spider: <code>scrapy crawl homeweather</code></p>\n</font>\n<h2 id=\"紧急\"><a class=\"anchor\" href=\"#紧急\">#</a> 紧急</h2>\n<p>学校要提前放假，考试也提前了。。<br />\n抓紧时间备考了</p>\n<p>考试后再总结</p>\n",
            "tags": [
                "scrapy"
            ]
        }
    ]
}